# -*- coding: utf-8 -*-
"""交大_資料探勘期末專題

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17cJLXGyTa524JO-hcQtBL_8AOZTyuZAD
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pylab
import seaborn as sns
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import LabelEncoder

sns.set_style("whitegrid")

df = pd.read_csv('/content/drive/My Drive/No-show-Issue-Comma-300k.csv')
print(df.head())

df.info()

df.describe()

df.isnull().sum()

df.drop(columns=['Unnamed: 4', 'Unnamed: 17', 'Unnamed: 18'],axis=1,inplace=True)
df

df.rename(columns = {'ApointmentData':'AppointmentData',
                         'Alcoolism': 'Alchoholism',
                         'HiperTension': 'Hypertension',
                         'Handcap': 'Handicap'}, inplace = True)

print(df.columns)

df.isnull().sum()

ax = sns.countplot(x=df.rain, hue=df.Status, data=df)
ax.set_title("Show/NoShow for Rain")
x_ticks_labels=['No Rain', 'Rain']
ax.set_xticklabels(x_ticks_labels)
plt.show()

#HeatMap
cor = df.corr()
cor

sns.heatmap(cor)

df.AppointmentRegistration = df.AppointmentRegistration.apply(np.datetime64)
df.AppointmentData = df.AppointmentData.apply(np.datetime64)
df.AwaitingTime = df.AwaitingTime.apply(abs)

print(df.AppointmentRegistration.head())
print(df.AppointmentData.head())
print(df.AwaitingTime.head())

normalizer = Normalizer('max')
features_age = normalizer.fit_transform(df[['Age']].T.values)
df['Age'] = features_age.T
features_AwaitingTime = normalizer.fit_transform(df[['AwaitingTime']].T.values)
df['AwaitingTime'] = features_AwaitingTime.T
df

# col_name = ['Age','AwaitingTime']
# features = df[col_name]
# normalizer = Normalizer(norm='max')
# features = normalizer.fit_transform(features.values)
# df[col_name] = features
# df

df.drop('AppointmentData',axis=1,inplace=True)
df.drop('AppointmentRegistration',axis=1,inplace=True)

df.boxplot()

le = LabelEncoder()
df['Gender'] = le.fit_transform(df['Gender'])
df['DayOfTheWeek'] = le.fit_transform(df['DayOfTheWeek'])
df['Status'] = le.fit_transform(df['Status'])
df

from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.metrics import roc_curve, roc_auc_score, auc, accuracy_score, confusion_matrix, classification_report
import pydotplus
from IPython.display import Image

X = df.drop(['Status'],axis=1)
y = df.Status
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)

#GaussianNB/13columns
from sklearn.naive_bayes import GaussianNB
g_nb = GaussianNB()
g_nb_fit = g_nb.fit(X_train,y_train)
print('Accuracy:',g_nb.score(X_test,y_test))
g_nb_pred = g_nb.predict(X_test)

print(confusion_matrix(y_test,g_nb_pred))
print('\n')
print(classification_report(y_test,g_nb_pred))

# from sklearn.preprocessing import StandardScaler
# col_names = ['Age']
# features = X[col_names]
# scaler = StandardScaler().fit(features.values)
# features = scaler.transform(features.values)
# X[col_names] = features
# X

!pip install info_gain
from info_gain import info_gain

#info_gain,Gain_Ratio/13columns
info_gain_list = []
info_gain_ratio_list = []

noShow_info = df.drop('Status',axis=1)
for column in noShow_info:
  ig = info_gain.info_gain(df[column], df['Status'])
  igr = info_gain.info_gain_ratio(df[column], df['Status'])
  info_gain_list.append(ig)
  info_gain_ratio_list.append(igr)

  print("%s的info_gain:" %(column),ig)
  print("%s的Gain_Ratio:" %(column),igr)

info_gain_list.sort(reverse=True)
info_gain_ratio_list.sort(reverse=True)

print(info_gain_list)

#DecisionTree/13columns
dt = tree.DecisionTreeClassifier(max_depth=10)
dt.fit(X_train,y_train)
print('Accuracy:',dt.score(X_test,y_test))
cnf=confusion_matrix(y_test,dt.predict(X_test))
print('混淆矩陣: \n',cnf)
print(classification_report(y_test,dt.predict(X_test)))
# tree.plot_tree(dt, filled=True)

# tree.plot_tree(dt, filled=True)

from sklearn.tree import export_graphviz
dot_data = tree.export_graphviz(dt,max_depth=5, out_file=None, filled=True, rounded=True, special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())

#隨機森林/13columns
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=10)
rfc.fit(X_train,y_train)
rfc_pred = rfc.predict(X_test)

print('Accuracy:',rfc.score(X_test,y_test))
print(confusion_matrix(y_test,rfc_pred))
print(classification_report(y_test,rfc_pred))

X.columns

# import six
# import os
# dotfile = six.StringIO()
# i_tree = 0
# for tree_in_forest in rfc.estimators_:
#   if (i_tree <1):        
#     tree.export_graphviz(tree_in_forest,
#                 feature_names=X.columns,
#                 filled=True,
#                 rounded=True)
#     os.system('dot -Tpng tree.dot -o tree.png')
#     pydotplus.graph_from_dot_data(dotfile.getvalue()).write_png('dtree'+ str(i_tree) +'.png')
#     i_tree = i_tree + 1

#KNN演算法/13columns
from sklearn.neighbors import KNeighborsClassifier
error_rate = []
for i in range(1,51):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(X_train,y_train)
  pred_i = knn.predict(X_test)
  error_rate.append(np.mean(pred_i != y_test))
  print(confusion_matrix(y_test,pred_i))
  print(classification_report(y_test,pred_i))

plt.figure(figsize=(10,6))
plt.plot(range(1,51),error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize='10')
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

# #roc curve / 13columns

# #random forest
# rf = RandomForestClassifier(n_estimators=10)
# rf.fit(X_train, y_train)
# y_pred_rf = rf.predict_proba(X_test)[:, 1]
# fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)
# auc_rf = auc(fpr_rf, tpr_rf)

# #貝氏
# gnb = GaussianNB()
# gnb.fit(X_train, y_train)
# y_pred_gnb = gnb.predict_proba(X_test)[:, 1]
# fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(y_test, y_pred_gnb)
# auc_gnb = auc(fpr_gnb, tpr_gnb)

# #KNN
# knn = KNeighborsClassifier(n_neighbors=50)
# knn.fit(X_train, y_train)
# y_pred_knn = knn.predict_proba(X_test)[:, 1]
# fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_pred_knn)
# auc_knn = auc(fpr_knn, tpr_knn)

# #plot roc curve
# plt.figure(1)
# plt.plot([0, 1], [0, 1], 'k--')
# plt.plot(fpr_gnb, tpr_gnb, label='GaussianNB (area={:.3f})'.format(auc_gnb))
# plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
# plt.plot(fpr_knn, tpr_knn, label='KNN (area = {:.3f})'.format(auc_knn))
# plt.xlabel('False positive rate')
# plt.ylabel('True positive rate')
# plt.title('ROC Curve')
# plt.legend(loc='best')
# plt.show()

#random forest
rf = RandomForestClassifier(n_estimators=10)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)
auc_rf = auc(fpr_rf, tpr_rf)

#DT
dt = tree.DecisionTreeClassifier(max_depth=10)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict_proba(X_test)[:, 1]
fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_dt)
auc_dt = auc(fpr_dt, tpr_dt)

#KNN
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict_proba(X_test)[:, 1]
fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_pred_knn)
auc_knn = auc(fpr_knn, tpr_knn)

#plot roc curve
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_dt, tpr_dt, label='DT (area={:.3f})'.format(auc_dt))
plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
plt.plot(fpr_knn, tpr_knn, label='KNN (area = {:.3f})'.format(auc_knn))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as opt
from torch.autograd import Variable

class RNN(nn.Module):
  def __init__(self):
    super(RNN, self).__init__()
    
    ### New layers:
    self.lstm = nn.LSTM(13 ,8, 1, batch_first = True)
    self.linear = nn.Linear(8, 1)
    self.sigmoid = nn.Sigmoid()
        
  def forward(self,x):
    out, (h,c) = self.lstm(x)
    result = self.linear(out)
    result = self.sigmoid(result)
    return result

model = RNN()
model

loss_fn = nn.BCELoss()
optimizer = opt.Adam(model.parameters(),lr=0.005)

num_epochs = 10
running_loss = 0.0
batch_size = 10000
for epoch in range(num_epochs):
  running_loss = 0.0
  for i in range(0, len(X_train), batch_size):
    # print(i)
    data = torch.tensor(np.array(X_train[i: i+10000]))[None].to(torch.float32)
    output = model(data)
    # output = output.gt(0.5).float()
    # loss = loss_fn(output.flatten(), torch.tensor(y_train.iloc[i], dtype=torch.float32)[None])
    loss = loss_fn(output.flatten(), torch.tensor(np.array(y_train[i: i+10000])).to(torch.float32))
    # print(loss)
    # backward
    optimizer.zero_grad()
    loss.backward()
    running_loss += loss.item()
    # gradient descent update step/adam step
    optimizer.step() 
  print('第{}個Epoch的Loss：{}'.format(epoch+1, running_loss/21))

correct = 0
total = 0
for i in range (len(X_test)):
  data = torch.tensor(X_test.iloc[i])[None, None].to(dtype=torch.float32)
  outputs = model(data)
  predicted = outputs.gt(0.5).int()
  # print(int(predicted))
  # print(y_test.iloc[i])
  correct += (int(predicted) == y_test.iloc[i]).sum()
total = len(y_test)
print(correct)
print(total)
print("Accr: %.3f%%"%(100.0 * float(correct)//float(total)))

# print(torch.tensor(X_train.iloc[3])[None, None].to(dtype=torch.float32))

# model(torch.tensor(X_train.iloc[3])[None, None].to(dtype=torch.float32))

# data = torch.tensor(X_train.iloc[0])[None, None].to(dtype=torch.float32)
# output = model(data)
# # output = output.gt(0.5).float()
# # loss = loss_fn(output.flatten(), torch.tensor(y_train.iloc[i], dtype=torch.float32)[None])
# loss = loss_fn(output.flatten(), torch.tensor(y_train.iloc[0], dtype=torch.float32)[None])
# print(loss.data)
# # backward
# optimizer.zero_grad()
# loss.backward()

# # gradient descent update step/adam step
# optimizer.step()

# torch.tensor(np.array(X_train.iloc[:1000]))[None].shape

# data = torch.tensor(np.array(X_train[:1000]))[None].to(torch.float32)
# output = model(data)
# # output = output.gt(0.5).float()
# # loss = loss_fn(output.flatten(), torch.tensor(y_train.iloc[i], dtype=torch.float32)[None])
# loss = loss_fn(output.flatten(), torch.tensor(np.array(y_train[:1000])).to(torch.float32))
# print(loss)
# # backward
# optimizer.zero_grad()
# loss.backward()

# # gradient descent update step/adam step
# optimizer.step()

# torch.tensor(np.array(X_train)).shape

# torch.tensor(np.array(X_train))[None]

# torch.tensor(np.array(y_train)).shape

