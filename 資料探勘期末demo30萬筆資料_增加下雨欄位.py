# -*- coding: utf-8 -*-
"""資料探勘期末Demo30萬筆資料_增加下雨欄位.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ibWfiYL5-ip1yv0Jnpnwhqfu1WEwZ5ky
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pds
import matplotlib.pyplot as plt
from matplotlib import pylab
import seaborn as sns
sns.set_style("whitegrid")

noShow = pds.read_csv('/content/drive/My Drive/No-show-Issue-Comma-300k.csv')
print(noShow.head())

noShow.drop('Unnamed: 17',axis=1,inplace=True)
noShow.drop('Unnamed: 18',axis=1,inplace=True)

noShow.head()

noShow.drop('Unnamed: 4',axis=1,inplace=True)
noShow.head()

noShow.rename(columns = {'ApointmentData':'AppointmentData',
                         'Alcoolism': 'Alchoholism',
                         'HiperTension': 'Hypertension',
                         'Handcap': 'Handicap'}, inplace = True)

print(noShow.columns)

noShow.isnull().sum()

noShow

noShow.drop(30000,axis=0,inplace=True)

noShow

noShow.isnull().sum()

noShow.info()

noShow

ax = sns.countplot(x=noShow.rain, hue=noShow.Status, data=noShow)
ax.set_title("Show/NoShow for Rain")
x_ticks_labels=['No Rain', 'Rain']
ax.set_xticklabels(x_ticks_labels)
plt.show()

#HeatMap
cor = noShow.corr()
print(cor)
_ = sns.heatmap(cor)

noShow.AppointmentRegistration = noShow.AppointmentRegistration.apply(np.datetime64)
noShow.AppointmentData = noShow.AppointmentData.apply(np.datetime64)
noShow.AwaitingTime = noShow.AwaitingTime.apply(abs)

print(noShow.AppointmentRegistration.head())
print(noShow.AppointmentData.head())
print(noShow.AwaitingTime.head())

from sklearn.preprocessing import Normalizer
col_name = ['Age','AwaitingTime']
features = noShow[col_name]
normalizer = Normalizer(norm='max')
features = normalizer.fit_transform(features.values)
noShow[col_name] = features
noShow

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
noShow['Gender'] = le.fit_transform(noShow['Gender'])
noShow['DayOfTheWeek'] = le.fit_transform(noShow['DayOfTheWeek'])
noShow['AppointmentData'] = le.fit_transform(noShow['AppointmentData'])
noShow['AppointmentRegistration'] = le.fit_transform(noShow['AppointmentRegistration'])
noShow['Status'] = le.fit_transform(noShow['Status'])
noShow

noShow.drop('AppointmentData',axis=1,inplace=True)
noShow.drop('AppointmentRegistration',axis=1,inplace=True)

from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.metrics import roc_curve,roc_auc_score,auc,accuracy_score,confusion_matrix,classification_report
import pydotplus
from IPython.display import Image

x = noShow.drop(['Status'],axis=1)
y = noShow.Status
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)

#GaussianNB/14columns
from sklearn.naive_bayes import GaussianNB
g_nb = GaussianNB(priors = None)
g_nb_fit = g_nb.fit(x_train,y_train)

g_nb_pred = g_nb.predict(x_test)
print(confusion_matrix(y_test,g_nb_pred))
print('\n')
print(classification_report(y_test,g_nb_pred))

from sklearn.preprocessing import StandardScaler
col_names = ['Age']
features = x[col_names]
scaler = StandardScaler().fit(features.values)
features = scaler.transform(features.values)
x[col_names] = features
x

#GaussianNB/16columns
from sklearn.naive_bayes import GaussianNB
g_nb = GaussianNB(priors = None)
g_nb_fit = g_nb.fit(x_train,y_train)

g_nb_pred = g_nb.predict(x_test)
print(confusion_matrix(y_test,g_nb_pred))
print('\n')
print(classification_report(y_test,g_nb_pred))

#info_gain,Gain_Ratio/14columns
!pip install info_gain
from info_gain import info_gain
noShow_plus = noShow.drop('Status',axis=1)
for item in noShow_plus:
  ig = info_gain.info_gain(noShow[item], noShow['Status'])
  igr = info_gain.info_gain_ratio(noShow[item], noShow['Status'])

  print("%s的info_gain:" %(item),ig)
  print("%s的Gain_Ratio:" %(item),igr)

#info_gain,Gain_Ratio/16columns
!pip install info_gain
from info_gain import info_gain
noShow_plus = noShow.drop('Status',axis=1)
for item in noShow_plus:
  ig = info_gain.info_gain(noShow[item], noShow['Status'])
  igr = info_gain.info_gain_ratio(noShow[item], noShow['Status'])

  print("%s的info_gain:" %(item),ig)
  print("%s的Gain_Ratio:" %(item),igr)

#DecisionTree/14columns
for i in range(10):
  dt = tree.DecisionTreeClassifier(max_depth = i+1)
  dt.fit(x_train,y_train)
  print('DecisionTree:')
  print('訓練集準確率:',dt.score(x_train,y_train))
  print('測試集準確率:',dt.score(x_test,y_test))

dt = tree.DecisionTreeClassifier(max_depth = 7)
dt.fit(x_train,y_train)
print('DecisionTree:')
print('訓練集準確率:',dt.score(x_train,y_train))
print('測試集準確率:',dt.score(x_test,y_test))
cnf=confusion_matrix(y_train,dt.predict(x_train))
print('訓練集混淆矩陣: \n',cnf)
cnf=confusion_matrix(y_test,dt.predict(x_test))
print('測試集混淆矩陣: \n',cnf)

tree.plot_tree(dt, filled=True)

#DecisionTree/16columns
for i in range(10):
  dt = tree.DecisionTreeClassifier(max_depth = i+1)
  dt.fit(x_train,y_train)
  print('DecisionTree:')
  print('訓練集準確率:',dt.score(x_train,y_train))
  print('測試集準確率:',dt.score(x_test,y_test))

dt = tree.DecisionTreeClassifier(max_depth = 7)
dt.fit(x_train,y_train)
print('DecisionTree:')
print('訓練集準確率:',dt.score(x_train,y_train))
print('測試集準確率:',dt.score(x_test,y_test))
cnf=confusion_matrix(y_train,dt.predict(x_train))
print('訓練集混淆矩陣: \n',cnf)
cnf=confusion_matrix(y_test,dt.predict(x_test))
print('測試集混淆矩陣: \n',cnf)

tree.plot_tree(dt, filled=True)

#14columns
from sklearn.tree import export_graphviz
dot_data = tree.export_graphviz(dt, out_file=None)
graph = pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())

#16columns
from sklearn.tree import export_graphviz
dot_data = tree.export_graphviz(dt, out_file=None,filled=True,rounded=True,special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())

#隨機森林/14columns
from sklearn.ensemble import RandomForestClassifier
for i in range(10):
  rfc = RandomForestClassifier(n_estimators = i+1)
  rfc.fit(x_train,y_train)
  rfc_pred = rfc.predict(x_test)
  print('n='+str(i+1))
  print(confusion_matrix(y_test,rfc_pred))
  print(classification_report(y_test,rfc_pred))

noShow

#隨機森林/16columns
from sklearn.ensemble import RandomForestClassifier
for i in range(10):
  rfc = RandomForestClassifier(n_estimators = i+1)
  rfc.fit(x_train,y_train)
  rfc_pred = rfc.predict(x_test)
  print('n='+str(i+1))
  print(confusion_matrix(y_test,rfc_pred))
  print(classification_report(y_test,rfc_pred))

#KNN演算法/14columns
from sklearn.neighbors import KNeighborsClassifier
error_rate = []
for i in range(1,51):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(x_train,y_train)
  pred_i = knn.predict(x_test)
  error_rate.append(np.mean(pred_i != y_test))
  print(confusion_matrix(y_test,pred_i))
  print(classification_report(y_test,pred_i))

plt.figure(figsize=(10,6))
plt.plot(range(1,51),error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize='10')
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

#KNN演算法/16columns
from sklearn.neighbors import KNeighborsClassifier
error_rate = []
for i in range(1,51):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(x_train,y_train)
  pred_i = knn.predict(x_test)
  error_rate.append(np.mean(pred_i != y_test))
  print(confusion_matrix(y_test,pred_i))
  print(classification_report(y_test,pred_i))

plt.figure(figsize=(10,6))
plt.plot(range(1,51),error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize='10')
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

#roc curve / 14columns

#random forest
rf = RandomForestClassifier(max_depth = 3, n_estimators=9)
rf.fit(x_train, y_train)
y_pred_rf = rf.predict_proba(x_test)[:, 1]
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)
auc_rf = auc(fpr_rf, tpr_rf)

#貝氏
gnb = GaussianNB(priors=None)
gnb.fit(x_train, y_train)
y_pred_gnb = gnb.predict_proba(x_test)[:, 1]
fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(y_test, y_pred_gnb)
auc_gnb = auc(fpr_gnb, tpr_gnb)

#KNN
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(x_train, y_train)
y_pred_knn = knn.predict_proba(x_test)[:, 1]
fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_pred_knn)
auc_knn = auc(fpr_knn, tpr_knn)

#plot roc curve
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_gnb, tpr_gnb, label='GaussianNB (area={:.3f})'.format(auc_gnb))
plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
plt.plot(fpr_knn, tpr_knn, label='KNN (area = {:.3f})'.format(auc_knn))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

#roc curve / 16columns

#random forest
rf = RandomForestClassifier(max_depth = 3, n_estimators=9)
rf.fit(x_train, y_train)
y_pred_rf = rf.predict_proba(x_test)[:, 1]
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)
auc_rf = auc(fpr_rf, tpr_rf)

#貝氏
gnb = GaussianNB(priors=None)
gnb.fit(x_train, y_train)
y_pred_gnb = gnb.predict_proba(x_test)[:, 1]
fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(y_test, y_pred_gnb)
auc_gnb = auc(fpr_gnb, tpr_gnb)

#KNN
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(x_train, y_train)
y_pred_knn = knn.predict_proba(x_test)[:, 1]
fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_pred_knn)
auc_knn = auc(fpr_knn, tpr_knn)

#plot roc curve
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_gnb, tpr_gnb, label='GaussianNB (area={:.3f})'.format(auc_gnb))
plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
plt.plot(fpr_knn, tpr_knn, label='KNN (area = {:.3f})'.format(auc_knn))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()